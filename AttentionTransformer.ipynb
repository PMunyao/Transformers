{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AttentionTransformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPL/baIbjARq4O08ShPX3A5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PMunyao/Transformers/blob/main/AttentionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "RN0GsbqXfZzd",
        "outputId": "54ee2b48-9630-49d7-c2fb-3dbe13d367b3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f11b62fc07d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;31m#train the 2D transformer on Plant Pathology 2020 - FGVC7 dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;31m#load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'plant-pathology-2020-fgvc7/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'plant-pathology-2020-fgvc7/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'plant-pathology-2020-fgvc7/train.csv'"
          ]
        }
      ],
      "source": [
        "#attention mechanism written in jax and stax libraries for 2D image data\n",
        "import jax.numpy as np\n",
        "from jax import random, grad, jit, vmap, lax, ops, jacrev\n",
        "from jax.experimental import stax, optimizers, ode\n",
        "from jax.experimental.stax import Dense, Relu, LogSoftmax, Dropout, Tanh\n",
        "from jax.tree_util import tree_flatten\n",
        "import pandas as pd\n",
        "\n",
        "class Attention(object):\n",
        "    \"\"\"\n",
        "    2D attention model using jax and stax libraries\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape, output_shape, kernel_size,\n",
        "                 num_filters, batch_size, attention_size,\n",
        "                 attention_layers, attention_dropout,\n",
        "                 final_dense_layer,\n",
        "                 optimizer, learning_rate,\n",
        "                 use_bias=True,\n",
        "                 use_edges=True,\n",
        "                 use_features=True,\n",
        "                 use_global_features=True,\n",
        "                 use_attention_dense_layer=True,\n",
        "                 use_attention_dropout=True):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_filters = num_filters\n",
        "        self.batch_size = batch_size\n",
        "        self.attention_size = attention_size\n",
        "        self.attention_layers = attention_layers\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.final_dense_layer = final_dense_layer\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.use_bias = use_bias\n",
        "        self.use_edges = use_edges\n",
        "        self.use_features = use_features\n",
        "        self.use_global_features = use_global_features\n",
        "        self.use_attention_dense_layer = use_attention_dense_layer\n",
        "        self.use_attention_dropout = use_attention_dropout\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"\n",
        "        Create the model\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Input\n",
        "        inputs = [\n",
        "            stax.Input(self.input_shape)\n",
        "        ]\n",
        "\n",
        "        # Edge convolution\n",
        "        if self.use_edges:\n",
        "            inputs.append(\n",
        "                stax.Input(self.input_shape)\n",
        "            )\n",
        "\n",
        "        # Feature maps\n",
        "        if self.use_features:\n",
        "            inputs.append(\n",
        "                stax.Input((self.input_shape[0], self.input_shape[1], self.input_shape[2]))\n",
        "            )\n",
        "\n",
        "        # Global features\n",
        "        if self.use_global_features:\n",
        "            inputs.append(\n",
        "                stax.Input((self.input_shape[0], self.input_shape[1], 1))\n",
        "            )\n",
        "\n",
        "        # Create the model\n",
        "        init_random_params, predict = stax.serial(\n",
        "            # Edge convolution\n",
        "            stax.Conv(self.num_filters, self.kernel_size,\n",
        "                      padding='SAME',\n",
        "                      filter_init=stax.ones,\n",
        "                      bias_init=stax.normal(0, 1)),\n",
        "            stax.Relu,\n",
        "            # Attention\n",
        "            stax.serial(\n",
        "                stax.Dense(self.attention_size,\n",
        "                           filter_init=stax.ones,\n",
        "                           bias_init=stax.normal(0, 1)),\n",
        "                stax.Relu,\n",
        "                stax.Dense(1,\n",
        "                           filter_init=stax.ones,\n",
        "                           bias_init=stax.normal(0, 1)),\n",
        "                stax.Softmax\n",
        "            ) if self.use_attention_dense_layer else stax.DotProduct(),\n",
        "            # Final convolution\n",
        "            stax.Conv(self.num_filters, self.kernel_size,\n",
        "                      padding='SAME',\n",
        "                      filter_init=stax.ones,\n",
        "                      bias_init=stax.normal(0, 1)),\n",
        "            stax.Relu,\n",
        "            # Final dense layer\n",
        "            stax.Dense(self.output_shape[0],\n",
        "                       filter_init=stax.ones,\n",
        "                       bias_init=stax.normal(0, 1)) if self.final_dense_layer else stax.serial(\n",
        "                stax.Flatten,\n",
        "                stax.Softmax\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Create the model\n",
        "        _, self.params = init_random_params(random.PRNGKey(0), inputs)\n",
        "\n",
        "        return predict\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Loss function\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :return: Loss\n",
        "        \"\"\"\n",
        "        # Predict the model\n",
        "        y_hat = self.model(X, self.params)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = -np.sum(ops.index_update(y, ops.index[:, 1], y_hat) * y) / self.batch_size\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def grad(self, X, y):\n",
        "        \"\"\"\n",
        "        Gradient of the loss function\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :return: Gradient of the loss\n",
        "        \"\"\"\n",
        "        return grad(self.loss)(X, y)\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"\n",
        "        Accuracy of the model\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :return: Accuracy\n",
        "        \"\"\"\n",
        "        # Predict the model\n",
        "        y_hat = self.model(X, self.params)\n",
        "\n",
        "        # Compute the accuracy\n",
        "        accuracy = np.sum(np.argmax(y, axis=1) == np.argmax(y_hat, axis=1)) / self.batch_size\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def fit(self, X, y, num_epochs=1000, print_loss=True):\n",
        "        \"\"\"\n",
        "        Fit the model\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :param num_epochs: Number of epochs\n",
        "        :param print_loss: Print the loss\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Optimizer\n",
        "        opt_init, opt_update, get_params = optimizers.sgd(self.learning_rate)\n",
        "\n",
        "        # Update function\n",
        "        opt_state = opt_init(self.params)\n",
        "        update_fn = lambda g: opt_update(g, get_params(opt_state))\n",
        "\n",
        "        # Train the model\n",
        "        for epoch in range(num_epochs):\n",
        "            # Update the parameters\n",
        "            opt_state = update_fn(self.grad(X, y))\n",
        "\n",
        "            # Print the loss\n",
        "            if print_loss and epoch % 100 == 0:\n",
        "                print('Loss: ' + str(self.loss(X, y)))\n",
        "\n",
        "        # Save the parameters\n",
        "        self.params = get_params(opt_state)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict using the model\n",
        "        :param X: Input\n",
        "        :return: Predictions\n",
        "        \"\"\"\n",
        "        return self.model(X, self.params)\n",
        "        \n",
        "class Transformers(object):\n",
        "    \"\"\"\n",
        "    Transformers for the 2D attention model\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape, output_shape, kernel_size, num_filters,\n",
        "                 use_bias=True,\n",
        "                 use_edges=True,\n",
        "                 use_features=True,\n",
        "                 use_global_features=True,\n",
        "                 use_attention_dense_layer=True,\n",
        "                 use_attention_dropout=True):\n",
        "        \"\"\"\n",
        "        Initialize the model\n",
        "        :param input_shape: Input shape\n",
        "        :param output_shape: Output shape\n",
        "        :param kernel_size: Kernel size\n",
        "        :param num_filters: Number of filters\n",
        "        :param use_bias: Use bias\n",
        "        :param use_edges: Use edges\n",
        "        :param use_features: Use features\n",
        "        :param use_global_features: Use global features\n",
        "        :param use_attention_dense_layer: Use attention dense layer\n",
        "        :param use_attention_dropout: Use attention dropout\n",
        "        \"\"\"\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_filters = num_filters\n",
        "        self.use_bias = use_bias\n",
        "        self.use_edges = use_edges\n",
        "        self.use_features = use_features\n",
        "        self.use_global_features = use_global_features\n",
        "        self.use_attention_dense_layer = use_attention_dense_layer\n",
        "        self.use_attention_dropout = use_attention_dropout\n",
        "        self.model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        \"\"\"\n",
        "        Create the model\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Input\n",
        "        inputs = [\n",
        "            stax.Input(self.input_shape)\n",
        "        ]\n",
        "\n",
        "        # Edge convolution\n",
        "        if self.use_edges:\n",
        "            inputs.append(\n",
        "                stax.Input(self.input_shape)\n",
        "            )\n",
        "\n",
        "        # Feature maps\n",
        "        if self.use_features:\n",
        "            inputs.append(\n",
        "                stax.Input((self.input_shape[0], self.input_shape[1], self.input_shape[2]))\n",
        "            )\n",
        "\n",
        "        # Global features\n",
        "        if self.use_global_features:\n",
        "            inputs.append(\n",
        "                stax.Input((self.input_shape[0], self.input_shape[1], 1))\n",
        "            )\n",
        "\n",
        "        # Create the model\n",
        "        init_random_params, predict = stax.serial(\n",
        "            # Edge convolution\n",
        "            stax.Conv(self.num_filters, self.kernel_size,\n",
        "                      padding='SAME',\n",
        "                      filter_init=stax.ones,\n",
        "                      bias_init=stax.normal(0, 1)),\n",
        "            stax.Relu,\n",
        "            # Attention\n",
        "            stax.serial(\n",
        "                stax.Dense(self.input_shape[0],\n",
        "                           filter_init=stax.ones,\n",
        "                           bias_init=stax.normal(0, 1)),\n",
        "                stax.Relu,\n",
        "                stax.Dense(1,\n",
        "                           filter_init=stax.ones,\n",
        "                           bias_init=stax.normal(0, 1)),\n",
        "                stax.Softmax\n",
        "            ) if self.use_attention_dense_layer else stax.DotProduct(),\n",
        "            # Final convolution\n",
        "            stax.Conv(self.num_filters, self.kernel_size,\n",
        "                      padding='SAME',\n",
        "                      filter_init=stax.ones,\n",
        "                      bias_init=stax.normal(0, 1)),\n",
        "            stax.Relu,\n",
        "            # Final dense layer\n",
        "            stax.Dense(self.output_shape[0],\n",
        "                       filter_init=stax.ones,\n",
        "                       bias_init=stax.normal(0, 1)) if self.final_dense_layer else stax.serial(\n",
        "                stax.Flatten,\n",
        "                stax.Softmax\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Create the model\n",
        "        _, self.params = init_random_params(random.PRNGKey(0), inputs)\n",
        "\n",
        "        return predict\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        \"\"\"\n",
        "        Loss function\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :return: Loss\n",
        "        \"\"\"\n",
        "        # Predict the model\n",
        "        y_hat = self.model(X, self.params)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = -np.sum(ops.index_update(y, ops.index[:, 1], y_hat) * y) / self.batch_size\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def grad(self, X, y):\n",
        "        \"\"\"\n",
        "        Gradient of the loss function\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :return: Gradient of the loss\n",
        "        \"\"\"\n",
        "        return grad(self.loss)(X, y)\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "        \"\"\"\n",
        "        Accuracy of the model\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :return: Accuracy\n",
        "        \"\"\"\n",
        "        # Predict the model\n",
        "        y_hat = self.model(X, self.params)\n",
        "\n",
        "        # Compute the accuracy\n",
        "        accuracy = np.sum(np.argmax(y, axis=1) == np.argmax(y_hat, axis=1)) / self.batch_size\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def fit(self, X, y, num_epochs=1000, print_loss=True):\n",
        "        \"\"\"\n",
        "        Fit the model\n",
        "        :param X: Input\n",
        "        :param y: Target\n",
        "        :param num_epochs: Number of epochs\n",
        "        :param print_loss: Print the loss\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # Optimizer\n",
        "        opt_init, opt_update, get_params = optimizers.sgd(self.learning_rate)\n",
        "\n",
        "        # Update function\n",
        "        opt_state = opt_init(self.params)\n",
        "        update_fn = lambda g: opt_update(g, get_params(opt_state))\n",
        "\n",
        "        # Train the model\n",
        "        for epoch in range(num_epochs):\n",
        "            # Update the parameters\n",
        "            opt_state = update_fn(self.grad(X, y))\n",
        "\n",
        "            # Print the loss\n",
        "            if print_loss and epoch % 100 == 0:\n",
        "                print('Loss: ' + str(self.loss(X, y)))\n",
        "\n",
        "        # Save the parameters\n",
        "        self.params = get_params(opt_state)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict using the model\n",
        "        :param X: Input\n",
        "        :return: Predictions\n",
        "        \"\"\"\n",
        "        return self.model(X, self.params)\n",
        "\n",
        "#train the 2D transformer on Plant Pathology 2020 - FGVC7 dataset\n",
        "#load the dataset\n",
        "train_df = pd.read_csv('plant-pathology-2020-fgvc7/train.csv')\n",
        "test_df = pd.read_csv('plant-pathology-2020-fgvc7/test.csv')\n",
        "\n",
        "#create a list of image file names\n",
        "train_imgs = train_df['image_id'].tolist()\n",
        "test_imgs = test_df['image_id'].tolist()\n",
        "\n",
        "#create a list of image labels\n",
        "train_labels = train_df[['healthy','multiple_diseases','rust','scab']].to_numpy()\n",
        "\n",
        "#load the images\n",
        "train_imgs, test_imgs = load_imgs(train_imgs, test_imgs)\n",
        "\n",
        "#normalize the images\n",
        "train_imgs = train_imgs/255.0\n",
        "test_imgs = test_imgs/255.0\n",
        "\n",
        "#create a list of image file names\n",
        "train_imgs = train_df['image_id'].tolist()\n",
        "test_imgs = test_df['image_id'].tolist()\n",
        "\n",
        "#create a list of image labels\n",
        "train_labels = train_df[['healthy','multiple_diseases','rust','scab']].to_numpy()\n",
        "\n",
        "#load the images\n",
        "train_imgs, test_imgs = load_imgs(train_imgs, test_imgs)\n",
        "\n",
        "#normalize the images\n",
        "train_imgs = train_imgs/255.0\n",
        "test_imgs = test_imgs/255.0\n",
        "\n",
        "#load the network\n",
        "network = Attention(input_shape=train_imgs[0].shape,\n",
        "                      output_shape=train_labels.shape[1],\n",
        "                      kernel_size=(3,3),\n",
        "                      num_filters=32,\n",
        "                      batch_size=32,\n",
        "                      attention_size=32,\n",
        "                      attention_layers=3,\n",
        "                      attention_dropout=0.1,\n",
        "                      final_dense_layer=True,\n",
        "                      optimizer='adam',\n",
        "                      learning_rate=0.001,\n",
        "                      use_bias=True,\n",
        "                      use_edges=True,\n",
        "                      use_features=True,\n",
        "                      use_global_features=True,\n",
        "                      use_attention_dense_layer=True,\n",
        "                      use_attention_dropout=True)\n",
        "\n",
        "#fit the model\n",
        "network.fit(train_imgs, train_labels, num_epochs=1000, print_loss=True)\n",
        "\n",
        "#predict the model\n",
        "predictions = network.predict(test_imgs)\n",
        "\n",
        "#create a submission file\n",
        "submission_df = pd.DataFrame(predictions, columns=['healthy','multiple_diseases','rust','scab'])\n",
        "submission_df.insert(0, 'image_id', test_imgs)\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "#save the model\n",
        "network.model.save('2d_attention_model.h5')\n",
        "\n",
        "#load the model\n",
        "network.model = tf.keras.models.load_model('2d_attention_model.h5')\n"
      ]
    }
  ]
}